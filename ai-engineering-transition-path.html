<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAQAAABLCVATAAAAdUlEQVR4Ae3MIRJAUBCH8e81R3AO53AmvIQmu5MzqE8iKoZAEdZYZhT7/cuGnR9/yeEJrDcW8DjEKlbFasQmFTQitj+cK2TqO8ggg2LceygiZ6YjeQel9Me10NI8ghR7DhlkkEFBxQTEShVUcVHGcAsZ8PylDecfc51kCzSlAAAAAElFTkSuQmCC type=image/x-icon><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wdth,wght@0,75..100,100..900;1,75..100,100..900&family=Ubuntu+Mono:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://ashishdoneriya.github.io/scss/main.min.css><title>AI Engineering Transition Path</title>
<meta name=description content="Research papers for software engineers to transition to AI Engineering"><meta name=author content="Ashish Doneriya"><meta name=robots content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"><link rel=canonical href=https://ashishdoneriya.github.io/ai-engineering-transition-path.html><meta property="og:type" content="article"><meta property="og:title" content="AI Engineering Transition Path"><meta property="og:description" content="Research papers for software engineers to transition to AI Engineering"><meta name=keywords content="technology,ai"><meta property="og:url" content="https://ashishdoneriya.github.io/ai-engineering-transition-path.html"><meta property="og:site_name" content="My Workbook"><meta property="article:published_time" content="2025-05-15T09:00:00+05:30"><meta property="article:modified_time" content="2025-05-15T09:00:00+05:30"><meta property="article:author" content="Ashish Doneriya"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="AI Engineering Transition Path"><meta name=twitter:description content="Research papers for software engineers to transition to AI Engineering"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"AI Engineering Transition Path","description":"Research papers for software engineers to transition to AI Engineering","author":{"@type":"Person","name":"Ashish Doneriya"},"keywords":["\"%!s(MISSING)\", \"%!s(MISSING)\""],"articleSection":["\"%!s(MISSING)\""],"datePublished":"2025-05-15T09:00:00\u002b05:30","dateModified":"2025-05-15T09:00:00\u002b05:30","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/ashishdoneriya.github.io\/ai-engineering-transition-path.html"},"url":"https:\/\/ashishdoneriya.github.io\/ai-engineering-transition-path.html"}</script></head><body><nav class="navbar navbar-expand-lg navbar-dark bg-dark"><div class=container><a class=navbar-brand href=/>My Workbook</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse justify-content-end" id=navbarNav><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/topics/big-data/>Big Data</a></li><li class=nav-item><a class=nav-link href=/topics/english/>English</a></li><li class=nav-item><a class=nav-link href=/topics/finance/>Finance</a></li><li class=nav-item><a class=nav-link href=/topics/interview/>Interview</a></li><li class=nav-item><a class=nav-link href=/topics/java/>Java</a></li><li class=nav-item><a class=nav-link href=/topics/lists/>Lists</a></li><li class=nav-item><a class=nav-link href=/topics/technology/>Technology</a></li></ul></div></div></nav><main><article class=page-wrapper><h1 class="mb-4 text-center">AI Engineering Transition Path</h1><div class="d-flex justify-content-center align-items-center mb-4 text-muted small"><time datetime="2025-05-15 09:00:00 +0530 +0530">May 15, 2025</time>
<span class=mx-2>|</span>
<span>Ashish Doneriya</span>
<span class=mx-2>|</span>
<a class=text-decoration-none href=/topics/technology rel=category>Technology</a></div><div class=post-content><p><strong>NOTE : This content is presented exactly as it appears in InterviewReady&rsquo;s AI Engineering Transition Path on <a href=https://github.com/InterviewReady/ai-engineering-resources target=_blank rel="noopener nofollow noreferrer">GitHub</a>. All credit goes to the original authors.</strong></p><p>Research papers for software engineers to transition to AI Engineering</p><h2 id=tokenization>Tokenization</h2><ul><li><a href=https://arxiv.org/pdf/1508.07909 target=_blank rel="noopener nofollow noreferrer">Byte-pair Encoding</a></li><li><a href=https://arxiv.org/pdf/2412.09871 target=_blank rel="noopener nofollow noreferrer">Byte Latent Transformer: Patches Scale Better Than Tokens</a></li></ul><h2 id=vectorization>Vectorization</h2><ul><li><a href=https://arxiv.org/pdf/1810.04805 target=_blank rel="noopener nofollow noreferrer">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href=https://arxiv.org/pdf/2305.05665 target=_blank rel="noopener nofollow noreferrer">IMAGEBIND: One Embedding Space To Bind Them All</a></li><li><a href=https://arxiv.org/pdf/2308.11466 target=_blank rel="noopener nofollow noreferrer">SONAR: Sentence-Level Multimodal and Language-Agnostic Representations</a></li><li><a href=https://arxiv.org/pdf/2401.08281 target=_blank rel="noopener nofollow noreferrer">FAISS library</a></li><li><a href=https://arxiv.org/pdf/2412.08821v2 target=_blank rel="noopener nofollow noreferrer">Facebook Large Concept Models</a></li></ul><h2 id=infrastructure>Infrastructure</h2><ul><li><a href=https://arxiv.org/pdf/1605.08695 target=_blank rel="noopener nofollow noreferrer">TensorFlow</a></li><li><a href=https://github.com/deepseek-ai/3FS/blob/main/docs/design_notes.md target=_blank rel="noopener nofollow noreferrer">Deepseek filesystem</a></li><li><a href=https://www.cs.purdue.edu/homes/csjgwang/pubs/SIGMOD21_Milvus.pdf target=_blank rel="noopener nofollow noreferrer">Milvus DB</a></li><li><a href=https://arxiv.org/pdf/1702.08734 target=_blank rel="noopener nofollow noreferrer">Billion Scale Similarity Search : FAISS</a></li><li><a href=https://arxiv.org/abs/1712.05889 target=_blank rel="noopener nofollow noreferrer">Ray</a></li></ul><h2 id=core-architecture>Core Architecture</h2><ul><li><a href=https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf target=_blank rel="noopener nofollow noreferrer">Attention is All You Need</a></li><li><a href=https://arxiv.org/pdf/2205.14135 target=_blank rel="noopener nofollow noreferrer">FlashAttention</a></li><li><a href=https://arxiv.org/pdf/1911.02150 target=_blank rel="noopener nofollow noreferrer">Multi Query Attention</a></li><li><a href=https://arxiv.org/pdf/2305.13245 target=_blank rel="noopener nofollow noreferrer">Grouped Query Attention</a></li><li><a href=https://arxiv.org/pdf/2501.00663 target=_blank rel="noopener nofollow noreferrer">Google Titans outperform Transformers</a></li><li><a href=https://arxiv.org/pdf/2502.05173 target=_blank rel="noopener nofollow noreferrer">VideoRoPE: Rotary Position Embedding</a></li></ul><h2 id=mixture-of-experts>Mixture of Experts</h2><ul><li><a href=https://arxiv.org/pdf/1701.06538 target=_blank rel="noopener nofollow noreferrer">Sparsely-Gated Mixture-of-Experts Layer</a></li><li><a href=https://arxiv.org/abs/2006.16668 target=_blank rel="noopener nofollow noreferrer">GShard</a></li><li><a href=https://arxiv.org/abs/2101.03961 target=_blank rel="noopener nofollow noreferrer">Switch Transformers</a></li></ul><h2 id=rlhf>RLHF</h2><ul><li><a href=https://arxiv.org/pdf/1706.03741 target=_blank rel="noopener nofollow noreferrer">Deep Reinforcement Learning with Human Feedback</a></li><li><a href=https://arxiv.org/pdf/1909.08593 target=_blank rel="noopener nofollow noreferrer">Fine-Tuning Language Models with RHLF</a></li><li><a href=https://arxiv.org/pdf/2203.02155 target=_blank rel="noopener nofollow noreferrer">Training language models with RHLF</a></li></ul><h2 id=chain-of-thought>Chain of Thought</h2><ul><li><a href=https://arxiv.org/pdf/2201.11903 target=_blank rel="noopener nofollow noreferrer">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2411.14405v1/ target=_blank rel="noopener nofollow noreferrer">Chain of thought</a></li><li><a href=https://arxiv.org/pdf/2502.03373 target=_blank rel="noopener nofollow noreferrer">Demystifying Long Chain-of-Thought Reasoning in LLMs</a></li></ul><h2 id=reasoning>Reasoning</h2><ul><li><a href=https://arxiv.org/pdf/2405.18512 target=_blank rel="noopener nofollow noreferrer">Transformer Reasoning Capabilities</a></li><li><a href=https://arxiv.org/pdf/2407.21787 target=_blank rel="noopener nofollow noreferrer">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</a></li><li><a href=https://arxiv.org/pdf/2408.03314 target=_blank rel="noopener nofollow noreferrer">Scale model test times is better than scaling parameters</a></li><li><a href=https://arxiv.org/pdf/2412.06769 target=_blank rel="noopener nofollow noreferrer">Training Large Language Models to Reason in a Continuous Latent Space</a></li><li><a href=https://arxiv.org/pdf/2501.12948v1 target=_blank rel="noopener nofollow noreferrer">DeepSeek R1</a></li><li><a href=https://arxiv.org/pdf/2502.01618 target=_blank rel="noopener nofollow noreferrer">A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods</a></li><li><a href=https://arxiv.org/pdf/2502.05171 target=_blank rel="noopener nofollow noreferrer">Latent Reasoning: A Recurrent Depth Approach</a></li><li><a href=https://arxiv.org/pdf/2504.13139 target=_blank rel="noopener nofollow noreferrer">Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo</a></li></ul><h2 id=optimizations>Optimizations</h2><ul><li><a href=https://arxiv.org/pdf/2402.17764 target=_blank rel="noopener nofollow noreferrer">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li><li><a href=https://arxiv.org/pdf/2407.08608 target=_blank rel="noopener nofollow noreferrer">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</a></li><li><a href=https://arxiv.org/pdf/2412.18653v1 target=_blank rel="noopener nofollow noreferrer">ByteDance 1.58</a></li><li><a href=https://arxiv.org/pdf/2501.06252 target=_blank rel="noopener nofollow noreferrer">Transformer Square</a></li><li><a href=https://arxiv.org/pdf/2501.09732 target=_blank rel="noopener nofollow noreferrer">Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps</a></li><li><a href=https://arxiv.org/pdf/2502.06703 target=_blank rel="noopener nofollow noreferrer">1b outperforms 405b</a></li><li><a href=https://arxiv.org/pdf/2211.17192 target=_blank rel="noopener nofollow noreferrer">Speculative Decoding</a></li></ul><h2 id=distillation>Distillation</h2><ul><li><a href=https://arxiv.org/pdf/1503.02531 target=_blank rel="noopener nofollow noreferrer">Distilling the Knowledge in a Neural Network</a></li><li><a href=https://arxiv.org/pdf/2006.07733 target=_blank rel="noopener nofollow noreferrer">BYOL - Distilled Architecture</a></li><li><a href=https://arxiv.org/pdf/2104.14294 target=_blank rel="noopener nofollow noreferrer">DINO</a></li></ul><h2 id=ssms>SSMs</h2><ul><li><a href=https://arxiv.org/pdf/2305.13048 target=_blank rel="noopener nofollow noreferrer">RWKV: Reinventing RNNs for the Transformer Era</a></li><li><a href=https://arxiv.org/pdf/2312.00752 target=_blank rel="noopener nofollow noreferrer">Mamba</a></li><li><a href=https://arxiv.org/pdf/2405.21060 target=_blank rel="noopener nofollow noreferrer">Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality</a></li><li><a href=https://arxiv.org/pdf/2408.10189 target=_blank rel="noopener nofollow noreferrer">Distilling Transformers to SSMs</a></li><li><a href=https://arxiv.org/pdf/2410.10254 target=_blank rel="noopener nofollow noreferrer">LoLCATs: On Low-Rank Linearizing of Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2502.20339 target=_blank rel="noopener nofollow noreferrer">Think Slow, Fast</a></li></ul><h2 id=competition-models>Competition Models</h2><ul><li><a href=https://arxiv.org/pdf/2502.03544 target=_blank rel="noopener nofollow noreferrer">Google Math Olympiad 2</a></li><li><a href=https://arxiv.org/pdf/2502.06807 target=_blank rel="noopener nofollow noreferrer">Competitive Programming with Large Reasoning Models</a></li><li><a href=https://www.nature.com/articles/s41586-023-06747-5 target=_blank rel="noopener nofollow noreferrer">Google Math Olympiad 1</a></li></ul><h2 id=hype-makers>Hype Makers</h2><ul><li><a href=https://arxiv.org/pdf/2501.04682 target=_blank rel="noopener nofollow noreferrer">Can AI be made to think critically</a></li><li><a href=https://arxiv.org/pdf/2501.09891 target=_blank rel="noopener nofollow noreferrer">Evolving Deeper LLM Thinking</a></li><li><a href=https://arxiv.org/pdf/2502.07374 target=_blank rel="noopener nofollow noreferrer">LLMs Can Easily Learn to Reason from Demonstrations Structure</a></li></ul><h2 id=hype-breakers>Hype Breakers</h2><ul><li><a href=https://arxiv.org/pdf/2301.06627 target=_blank rel="noopener nofollow noreferrer">Separating communication from intelligence</a></li><li><a href=https://gwern.net/doc/psychology/linguistics/2024-fedorenko.pdf target=_blank rel="noopener nofollow noreferrer">Language is not intelligence</a></li></ul><h2 id=image-transformers>Image Transformers</h2><ul><li><a href=https://arxiv.org/pdf/2010.11929 target=_blank rel="noopener nofollow noreferrer">Image is 16x16 word</a></li><li><a href=https://arxiv.org/pdf/2103.00020 target=_blank rel="noopener nofollow noreferrer">CLIP</a></li><li><a href=https://arxiv.org/pdf/2501.17811 target=_blank rel="noopener nofollow noreferrer">deepseek image generation</a></li></ul><h2 id=video-transformers>Video Transformers</h2><ul><li><a href=https://arxiv.org/pdf/2103.15691 target=_blank rel="noopener nofollow noreferrer">ViViT: A Video Vision Transformer</a></li><li><a href=https://arxiv.org/pdf/2404.08471 target=_blank rel="noopener nofollow noreferrer">Joint Embedding abstractions with self-supervised video masks</a></li><li><a href=https://arxiv.org/pdf/2502.02492 target=_blank rel="noopener nofollow noreferrer">Facebook VideoJAM ai gen</a></li></ul><h2 id=case-studies>Case Studies</h2><ul><li><a href=https://arxiv.org/pdf/2402.09171 target=_blank rel="noopener nofollow noreferrer">Automated Unit Test Improvement using Large Language Models at Meta</a></li><li><a href=https://arxiv.org/pdf/2404.17723v1 target=_blank rel="noopener nofollow noreferrer">Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering</a></li><li><a href=https://arxiv.org/pdf/2412.16720 target=_blank rel="noopener nofollow noreferrer">OpenAI o1 System Card</a></li><li><a href=https://arxiv.org/pdf/2501.12862 target=_blank rel="noopener nofollow noreferrer">LLM-powered bug catchers</a></li><li><a href=https://arxiv.org/pdf/2501.14342 target=_blank rel="noopener nofollow noreferrer">Chain-of-Retrieval Augmented Generation</a></li><li><a href=https://bytes.swiggy.com/improving-search-relevance-in-hyperlocal-food-delivery-using-small-language-models-ecda2acc24e6 target=_blank rel="noopener nofollow noreferrer">Swiggy Search</a></li><li><a href=https://github.com/openai/swarm target=_blank rel="noopener nofollow noreferrer">Swarm by OpenAI</a></li><li><a href=https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39 target=_blank rel="noopener nofollow noreferrer">Netflix Foundation Models</a></li><li><a href=https://www.anthropic.com/news/model-context-protocol target=_blank rel="noopener nofollow noreferrer">Model Context Protocol</a></li><li><a href=https://www.uber.com/en-IN/blog/query-gpt/ target=_blank rel="noopener nofollow noreferrer">uber queryGPT</a></li></ul><h2 id=more-resources>More Resources</h2><p>I manage my lists here: <a href=https://interviewready.io/resources/ target=_blank rel="noopener nofollow noreferrer">https://interviewready.io/resources/</a></p></div></article></main><script async src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js defer></script></body></html>